{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continual Learning with Deep Artificial Neurons\n",
    "https://arxiv.org/pdf/2011.07035.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T00:59:39.864270Z",
     "iopub.status.busy": "2020-11-22T00:59:39.864270Z",
     "iopub.status.idle": "2020-11-22T00:59:39.867262Z",
     "shell.execute_reply": "2020-11-22T00:59:39.867262Z",
     "shell.execute_reply.started": "2020-11-22T00:59:39.864270Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:29:10.469381Z",
     "iopub.status.busy": "2020-11-22T05:29:10.469381Z",
     "iopub.status.idle": "2020-11-22T05:29:10.477392Z",
     "shell.execute_reply": "2020-11-22T05:29:10.477392Z",
     "shell.execute_reply.started": "2020-11-22T05:29:10.469381Z"
    }
   },
   "outputs": [],
   "source": [
    "class Neuron(nn.Module):\n",
    "    \"\"\"\n",
    "    Core of a single neural component. This can be thought of as a simple 2-layer ANN\n",
    "    \n",
    "    ::param ni: input layer dimension\n",
    "    ::param nf: intermediate layer dimension\n",
    "    ::nout_ni: next Neuron input layer dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, nf, nout_ni, bias=True, act_fn=None):\n",
    "        super(Neuron, self).__init__()\n",
    "        self._ni = ni\n",
    "        act_fn = nn.Tanh() if act_fn is None else act_fn\n",
    "        assert isinstance(act_fn, nn.Module), 'activation must be of nn.Module instance'\n",
    "        \n",
    "        lin1 = nn.Linear(ni, nf, bias=bias)\n",
    "        lin2 = nn.Linear(nf, nf, bias=bias)\n",
    "        lin3 = nn.Linear(nf, nout_ni, bias=bias)\n",
    "        \n",
    "        bn1 = nn.BatchNorm1d(ni)\n",
    "        bn2 = nn.BatchNorm1d(nf)\n",
    "        \n",
    "        layers = [bn1, lin1, act_fn, bn2, lin2, act_fn, lin3]\n",
    "        \n",
    "        self.core = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self._ni, f'input to neuron ({x.shape[1]}) does not match input dimension size: ({self._ni})'\n",
    "        return self.core(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:29:11.504643Z",
     "iopub.status.busy": "2020-11-22T05:29:11.504643Z",
     "iopub.status.idle": "2020-11-22T05:29:11.512621Z",
     "shell.execute_reply": "2020-11-22T05:29:11.512621Z",
     "shell.execute_reply.started": "2020-11-22T05:29:11.504643Z"
    }
   },
   "outputs": [],
   "source": [
    "class DANLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single DANLayer is composed of n number of Neurons.\n",
    "    \n",
    "    ::param neurons: number of neurons in given layer\n",
    "    ::param n_ni: dendritic dimension, intermediate layers take concatenated output from previous layer\n",
    "    ::param n_nf: intermediate neuron dimension\n",
    "    ::param n_out_ni: axonal dimension\n",
    "    ::param p_neurons: number of neurons in previous layer \n",
    "    \"\"\"\n",
    "    def __init__(self, neurons, n_ni, n_nf, n_nout_ni, p_neurons=1, bias=True, act_fn=None):\n",
    "        super(DANLayer, self).__init__()\n",
    "        self._n = neurons\n",
    "        self._ni = n_ni*p_neurons\n",
    "        self.neurons = nn.ModuleList([Neuron(n_ni*p_neurons, n_nf, n_nout_ni, bias=bias, act_fn=act_fn) for _ in range(neurons)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ::param x: dimensions -> (batch, features)\n",
    "        for intermediate layers, features will be the concatenated output\n",
    "        \"\"\"\n",
    "        b, f = x.size()\n",
    "        assert f==self._ni, f'input dimension ({f}) do not match neuron input dimension ({self._ni})'\n",
    "        \n",
    "        outs = [self.neurons[i](x) for i in range(self._n)]\n",
    "        return torch.cat(outs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:29:11.592490Z",
     "iopub.status.busy": "2020-11-22T05:29:11.592490Z",
     "iopub.status.idle": "2020-11-22T05:29:11.600465Z",
     "shell.execute_reply": "2020-11-22T05:29:11.600465Z",
     "shell.execute_reply.started": "2020-11-22T05:29:11.592490Z"
    }
   },
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    \"\"\"\n",
    "    Hardcodes DAN architecture. This follows the paper https://arxiv.org/pdf/2011.07035.pdf\n",
    "    A simple 3 layer DAN\n",
    "    ::param ni: number of dimension for first DANLayer\n",
    "    ::param num_classes: number of classes to predict\n",
    "    \"\"\"\n",
    "    def __init__(self, ni, num_classes, act=None):\n",
    "        super(DAN, self).__init__()\n",
    "        \"\"\"\n",
    "        NOTE: n_out_ni must match next layers n_ni\n",
    "        TODO: make this dynamic\n",
    "        \"\"\"\n",
    "        self.dan1 = DANLayer(neurons=3, n_ni=ni, n_nf=50, n_nout_ni=100, p_neurons=1, act_fn=act)\n",
    "        self.dan2 = DANLayer(neurons=2, n_ni=100, n_nf=50, n_nout_ni=25, p_neurons=3, act_fn=act)\n",
    "        self.dan3 = DANLayer(neurons=1, n_ni=25, n_nf=10, n_nout_ni=num_classes, p_neurons=2, act_fn=act)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ::param x: shape (batch, features)\n",
    "        \"\"\"\n",
    "        x = self.dan1(x)\n",
    "        x = self.dan2(x)\n",
    "        return self.dan3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:29:13.095467Z",
     "iopub.status.busy": "2020-11-22T05:29:13.094496Z",
     "iopub.status.idle": "2020-11-22T05:29:13.102448Z",
     "shell.execute_reply": "2020-11-22T05:29:13.101449Z",
     "shell.execute_reply.started": "2020-11-22T05:29:13.095467Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs, model, opt, loss_fn, device='cuda:0'):\n",
    "    device = torch.device(device)\n",
    "    model = model.to(device)\n",
    "    dummy_data = [(torch.randn(64, 100), torch.randn(64, 1)) for i in range(100)]\n",
    "    for i in range(epochs):\n",
    "        for dd in dummy_data:\n",
    "            x, y = dd\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            loss = loss_fn(out, y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        print(f'Epoch: {i+1}/{epochs}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:29:14.524206Z",
     "iopub.status.busy": "2020-11-22T05:29:14.524206Z",
     "iopub.status.idle": "2020-11-22T05:29:14.527229Z",
     "shell.execute_reply": "2020-11-22T05:29:14.527229Z",
     "shell.execute_reply.started": "2020-11-22T05:29:14.524206Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:29:14.763567Z",
     "iopub.status.busy": "2020-11-22T05:29:14.763567Z",
     "iopub.status.idle": "2020-11-22T05:31:01.150338Z",
     "shell.execute_reply": "2020-11-22T05:31:01.150338Z",
     "shell.execute_reply.started": "2020-11-22T05:29:14.763567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, loss: 0.840283215045929\n",
      "Epoch: 2/100, loss: 0.758152961730957\n",
      "Epoch: 3/100, loss: 0.7313737869262695\n",
      "Epoch: 4/100, loss: 0.6388325691223145\n",
      "Epoch: 5/100, loss: 0.5919957160949707\n",
      "Epoch: 6/100, loss: 0.3751508891582489\n",
      "Epoch: 7/100, loss: 0.34337562322616577\n",
      "Epoch: 8/100, loss: 0.21609780192375183\n",
      "Epoch: 9/100, loss: 0.20937995612621307\n",
      "Epoch: 10/100, loss: 0.12115730345249176\n",
      "Epoch: 11/100, loss: 0.11588992178440094\n",
      "Epoch: 12/100, loss: 0.1057678684592247\n",
      "Epoch: 13/100, loss: 0.11734393239021301\n",
      "Epoch: 14/100, loss: 0.07674159109592438\n",
      "Epoch: 15/100, loss: 0.0907539576292038\n",
      "Epoch: 16/100, loss: 0.06652944535017014\n",
      "Epoch: 17/100, loss: 0.05083984136581421\n",
      "Epoch: 18/100, loss: 0.051033034920692444\n",
      "Epoch: 19/100, loss: 0.05533038079738617\n",
      "Epoch: 20/100, loss: 0.07391571253538132\n",
      "Epoch: 21/100, loss: 0.06154697760939598\n",
      "Epoch: 22/100, loss: 0.09060288220643997\n",
      "Epoch: 23/100, loss: 0.0421791598200798\n",
      "Epoch: 24/100, loss: 0.0698050707578659\n",
      "Epoch: 25/100, loss: 0.04799585044384003\n",
      "Epoch: 26/100, loss: 0.05263873189687729\n",
      "Epoch: 27/100, loss: 0.06746257096529007\n",
      "Epoch: 28/100, loss: 0.04351305589079857\n",
      "Epoch: 29/100, loss: 0.04782159626483917\n",
      "Epoch: 30/100, loss: 0.05799956992268562\n",
      "Epoch: 31/100, loss: 0.06950636953115463\n",
      "Epoch: 32/100, loss: 0.04491154104471207\n",
      "Epoch: 33/100, loss: 0.058590006083250046\n",
      "Epoch: 34/100, loss: 0.055088356137275696\n",
      "Epoch: 35/100, loss: 0.050582416355609894\n",
      "Epoch: 36/100, loss: 0.04955913871526718\n",
      "Epoch: 37/100, loss: 0.04671993851661682\n",
      "Epoch: 38/100, loss: 0.043965741991996765\n",
      "Epoch: 39/100, loss: 0.06489814817905426\n",
      "Epoch: 40/100, loss: 0.05230673775076866\n",
      "Epoch: 41/100, loss: 0.046913497149944305\n",
      "Epoch: 42/100, loss: 0.038295913487672806\n",
      "Epoch: 43/100, loss: 0.04009876027703285\n",
      "Epoch: 44/100, loss: 0.03212238475680351\n",
      "Epoch: 45/100, loss: 0.02860543690621853\n",
      "Epoch: 46/100, loss: 0.026370767503976822\n",
      "Epoch: 47/100, loss: 0.04225245863199234\n",
      "Epoch: 48/100, loss: 0.04282159358263016\n",
      "Epoch: 49/100, loss: 0.02469182014465332\n",
      "Epoch: 50/100, loss: 0.030157942324876785\n",
      "Epoch: 51/100, loss: 0.03323512524366379\n",
      "Epoch: 52/100, loss: 0.058574456721544266\n",
      "Epoch: 53/100, loss: 0.03661128506064415\n",
      "Epoch: 54/100, loss: 0.045030590146780014\n",
      "Epoch: 55/100, loss: 0.05512792617082596\n",
      "Epoch: 56/100, loss: 0.0591745488345623\n",
      "Epoch: 57/100, loss: 0.05935607850551605\n",
      "Epoch: 58/100, loss: 0.05828193575143814\n",
      "Epoch: 59/100, loss: 0.052126746624708176\n",
      "Epoch: 60/100, loss: 0.04448340833187103\n",
      "Epoch: 61/100, loss: 0.03309599682688713\n",
      "Epoch: 62/100, loss: 0.04540465772151947\n",
      "Epoch: 63/100, loss: 0.03342989832162857\n",
      "Epoch: 64/100, loss: 0.01738770119845867\n",
      "Epoch: 65/100, loss: 0.0227925106883049\n",
      "Epoch: 66/100, loss: 0.042144518345594406\n",
      "Epoch: 67/100, loss: 0.03692663088440895\n",
      "Epoch: 68/100, loss: 0.014190363697707653\n",
      "Epoch: 69/100, loss: 0.021632838994264603\n",
      "Epoch: 70/100, loss: 0.040620334446430206\n",
      "Epoch: 71/100, loss: 0.042182765901088715\n",
      "Epoch: 72/100, loss: 0.02444595843553543\n",
      "Epoch: 73/100, loss: 0.023461047559976578\n",
      "Epoch: 74/100, loss: 0.03813976049423218\n",
      "Epoch: 75/100, loss: 0.045133769512176514\n",
      "Epoch: 76/100, loss: 0.04377148300409317\n",
      "Epoch: 77/100, loss: 0.037624575197696686\n",
      "Epoch: 78/100, loss: 0.02721809409558773\n",
      "Epoch: 79/100, loss: 0.03636564314365387\n",
      "Epoch: 80/100, loss: 0.051210056990385056\n",
      "Epoch: 81/100, loss: 0.029561420902609825\n",
      "Epoch: 82/100, loss: 0.04574556648731232\n",
      "Epoch: 83/100, loss: 0.03740326687693596\n",
      "Epoch: 84/100, loss: 0.031348131597042084\n",
      "Epoch: 85/100, loss: 0.03265706077218056\n",
      "Epoch: 86/100, loss: 0.022246774286031723\n",
      "Epoch: 87/100, loss: 0.02331499010324478\n",
      "Epoch: 88/100, loss: 0.01621239259839058\n",
      "Epoch: 89/100, loss: 0.017777826637029648\n",
      "Epoch: 90/100, loss: 0.015583888627588749\n",
      "Epoch: 91/100, loss: 0.012847062200307846\n",
      "Epoch: 92/100, loss: 0.012734253890812397\n",
      "Epoch: 93/100, loss: 0.020708754658699036\n",
      "Epoch: 94/100, loss: 0.020721159875392914\n",
      "Epoch: 95/100, loss: 0.015323575586080551\n",
      "Epoch: 96/100, loss: 0.013243163004517555\n",
      "Epoch: 97/100, loss: 0.021922290325164795\n",
      "Epoch: 98/100, loss: 0.02201676368713379\n",
      "Epoch: 99/100, loss: 0.035103704780340195\n",
      "Epoch: 100/100, loss: 0.038852281868457794\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "model = DAN(100, 1, act=nn.ReLU())\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train(100, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:34:19.532469Z",
     "iopub.status.busy": "2020-11-22T05:34:19.531493Z",
     "iopub.status.idle": "2020-11-22T05:34:48.497634Z",
     "shell.execute_reply": "2020-11-22T05:34:48.497634Z",
     "shell.execute_reply.started": "2020-11-22T05:34:19.532469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100, loss: 0.9728431701660156\n",
      "Epoch: 2/100, loss: 0.828559398651123\n",
      "Epoch: 3/100, loss: 0.6443156003952026\n",
      "Epoch: 4/100, loss: 0.2395050972700119\n",
      "Epoch: 5/100, loss: 0.16950803995132446\n",
      "Epoch: 6/100, loss: 0.2235107123851776\n",
      "Epoch: 7/100, loss: 0.1715463399887085\n",
      "Epoch: 8/100, loss: 0.14578860998153687\n",
      "Epoch: 9/100, loss: 0.11118602752685547\n",
      "Epoch: 10/100, loss: 0.08071592450141907\n",
      "Epoch: 11/100, loss: 0.10418432950973511\n",
      "Epoch: 12/100, loss: 0.09245992451906204\n",
      "Epoch: 13/100, loss: 0.06690025329589844\n",
      "Epoch: 14/100, loss: 0.05668084695935249\n",
      "Epoch: 15/100, loss: 0.07833646237850189\n",
      "Epoch: 16/100, loss: 0.09935720264911652\n",
      "Epoch: 17/100, loss: 0.054995857179164886\n",
      "Epoch: 18/100, loss: 0.09818050265312195\n",
      "Epoch: 19/100, loss: 0.05502917245030403\n",
      "Epoch: 20/100, loss: 0.05882522091269493\n",
      "Epoch: 21/100, loss: 0.05111636966466904\n",
      "Epoch: 22/100, loss: 0.05144578218460083\n",
      "Epoch: 23/100, loss: 0.10784360021352768\n",
      "Epoch: 24/100, loss: 0.09862090647220612\n",
      "Epoch: 25/100, loss: 0.04022400826215744\n",
      "Epoch: 26/100, loss: 0.06333405524492264\n",
      "Epoch: 27/100, loss: 0.05774356424808502\n",
      "Epoch: 28/100, loss: 0.03318847715854645\n",
      "Epoch: 29/100, loss: 0.043922606855630875\n",
      "Epoch: 30/100, loss: 0.042756762355566025\n",
      "Epoch: 31/100, loss: 0.042522989213466644\n",
      "Epoch: 32/100, loss: 0.04934902489185333\n",
      "Epoch: 33/100, loss: 0.03919213265180588\n",
      "Epoch: 34/100, loss: 0.03522936999797821\n",
      "Epoch: 35/100, loss: 0.03699184209108353\n",
      "Epoch: 36/100, loss: 0.036848850548267365\n",
      "Epoch: 37/100, loss: 0.027072802186012268\n",
      "Epoch: 38/100, loss: 0.04050963371992111\n",
      "Epoch: 39/100, loss: 0.07732874900102615\n",
      "Epoch: 40/100, loss: 0.05726362019777298\n",
      "Epoch: 41/100, loss: 0.041070595383644104\n",
      "Epoch: 42/100, loss: 0.0419335663318634\n",
      "Epoch: 43/100, loss: 0.06143728643655777\n",
      "Epoch: 44/100, loss: 0.041835300624370575\n",
      "Epoch: 45/100, loss: 0.038889504969120026\n",
      "Epoch: 46/100, loss: 0.06453920155763626\n",
      "Epoch: 47/100, loss: 0.06322789192199707\n",
      "Epoch: 48/100, loss: 0.04385577142238617\n",
      "Epoch: 49/100, loss: 0.044474031776189804\n",
      "Epoch: 50/100, loss: 0.0534859225153923\n",
      "Epoch: 51/100, loss: 0.05456647276878357\n",
      "Epoch: 52/100, loss: 0.07573255896568298\n",
      "Epoch: 53/100, loss: 0.034737687557935715\n",
      "Epoch: 54/100, loss: 0.04016818106174469\n",
      "Epoch: 55/100, loss: 0.057874757796525955\n",
      "Epoch: 56/100, loss: 0.06827698647975922\n",
      "Epoch: 57/100, loss: 0.03814620524644852\n",
      "Epoch: 58/100, loss: 0.025234151631593704\n",
      "Epoch: 59/100, loss: 0.02884647436439991\n",
      "Epoch: 60/100, loss: 0.04287377744913101\n",
      "Epoch: 61/100, loss: 0.02989319898188114\n",
      "Epoch: 62/100, loss: 0.017003905028104782\n",
      "Epoch: 63/100, loss: 0.023115914314985275\n",
      "Epoch: 64/100, loss: 0.04697098582983017\n",
      "Epoch: 65/100, loss: 0.026767536997795105\n",
      "Epoch: 66/100, loss: 0.020986108109354973\n",
      "Epoch: 67/100, loss: 0.021469905972480774\n",
      "Epoch: 68/100, loss: 0.036960918456315994\n",
      "Epoch: 69/100, loss: 0.042665064334869385\n",
      "Epoch: 70/100, loss: 0.03579830378293991\n",
      "Epoch: 71/100, loss: 0.029843881726264954\n",
      "Epoch: 72/100, loss: 0.033014945685863495\n",
      "Epoch: 73/100, loss: 0.03547879308462143\n",
      "Epoch: 74/100, loss: 0.04481341689825058\n",
      "Epoch: 75/100, loss: 0.03252880275249481\n",
      "Epoch: 76/100, loss: 0.025250619277358055\n",
      "Epoch: 77/100, loss: 0.03448018059134483\n",
      "Epoch: 78/100, loss: 0.044429559260606766\n",
      "Epoch: 79/100, loss: 0.0325397253036499\n",
      "Epoch: 80/100, loss: 0.022085431963205338\n",
      "Epoch: 81/100, loss: 0.025225063785910606\n",
      "Epoch: 82/100, loss: 0.04209115356206894\n",
      "Epoch: 83/100, loss: 0.031263623386621475\n",
      "Epoch: 84/100, loss: 0.026744045317173004\n",
      "Epoch: 85/100, loss: 0.01899636723101139\n",
      "Epoch: 86/100, loss: 0.021807760000228882\n",
      "Epoch: 87/100, loss: 0.04980206862092018\n",
      "Epoch: 88/100, loss: 0.04980508238077164\n",
      "Epoch: 89/100, loss: 0.025920171290636063\n",
      "Epoch: 90/100, loss: 0.022020157426595688\n",
      "Epoch: 91/100, loss: 0.04191992059350014\n",
      "Epoch: 92/100, loss: 0.0362548902630806\n",
      "Epoch: 93/100, loss: 0.027478959411382675\n",
      "Epoch: 94/100, loss: 0.017722077667713165\n",
      "Epoch: 95/100, loss: 0.03559234365820885\n",
      "Epoch: 96/100, loss: 0.029211122542619705\n",
      "Epoch: 97/100, loss: 0.015230389311909676\n",
      "Epoch: 98/100, loss: 0.01207251287996769\n",
      "Epoch: 99/100, loss: 0.02738899551331997\n",
      "Epoch: 100/100, loss: 0.02799258753657341\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "\n",
    "basic_model = nn.Sequential(\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.Linear(100, 350),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(350),\n",
    "    nn.Linear(350, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.BatchNorm1d(100),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(100, 1)\n",
    ")\n",
    "\n",
    "optimizer = optim.Adam(basic_model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train(100, basic_model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:35:25.690265Z",
     "iopub.status.busy": "2020-11-22T05:35:25.689267Z",
     "iopub.status.idle": "2020-11-22T05:35:25.695251Z",
     "shell.execute_reply": "2020-11-22T05:35:25.695251Z",
     "shell.execute_reply.started": "2020-11-22T05:35:25.690265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78901"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-22T05:35:25.863807Z",
     "iopub.status.busy": "2020-11-22T05:35:25.862792Z",
     "iopub.status.idle": "2020-11-22T05:35:25.868757Z",
     "shell.execute_reply": "2020-11-22T05:35:25.867762Z",
     "shell.execute_reply.started": "2020-11-22T05:35:25.863807Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81751"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in basic_model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
